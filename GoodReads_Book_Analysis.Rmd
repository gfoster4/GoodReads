---
title: "GoodReads Dataset Analysis in R: An Exploration"
author: "Greg Foster"
date: "-March 2021-"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### *The primary goal of this project is to demonstrate the process of exploring a real world data set downloaded from Kaggle with R programming, and to share the results. Any discoveries made are aimed at forming future hypotheses regarding the nature of publication success for different variables, such as page number or publication date. Publication success is quantified here through average ratings and the number of written text reviews for each publication.*

#### *The secondary goal of this project is to demonstrate proficiency with R programming, R Markdown, and various statistical visualization methods.*

<<<<<<< HEAD
#### *After exploring the good reads data set in R, several interesting trends emerged. First, as book lengths exceed 500 pages and above, the range of average ratings decreases. This indicates that readers of longer books and collections are more pleased with their experience overall than those of shorter ones. Second, as book and collection lengths exceed 500 pages and above, the number of written reviews decreases. This adds some context to the first trend, as it suggests that despite having fewer readers, books with 500 pages and above have higher ratings. The people that actually get through these titles are reliably pleased with their experience. Another interesting finding was that portions of the data set were incorrect, discovered through a combination of multivariate plotting and sql queries.*

##### *The data for this project was obtained here: https://www.kaggle.com/jealousleopard/goodreadsbooks* 

## After some initial cleaning of the dataset in Excel, the exploration of the goodreads data in R can begin.
=======
## Setting the working directory
```{r}
setwd("~/Documents/Current_Classes/R_Programming/Internship Project")
```
>>>>>>> parent of 9156b0d (update)

## Importing the cleaned csv file
```{r}
books <- read.csv("books.csv")
head(books)
```


## Deleting the ISBN and ISBN13 columns
#### The ISBN numbers for any given title are tools used by publishers to keep track of book titles. Since we already have a title column for each book in the data set, these columns are redundant. 
```{r}
head(books[,5:6]) #Here are the columns to be deleted
books <- books[,-5:-6]
colnames(books) #Checking to make sure the two columns have been removed
```

## Initial Look into the Data
```{r}
summary(books)
```

### The publication date is in a character format, preventing proper analysis on that column. This can be fixed with the stringr package.
```{r}
typeof(books$publication_date)
head(books$publication_date)

library(stringr)

nchar(head(books$publication_date))
str_sub(books$publication_date, 5, 10) <- ""  #This removes the month and day values
books$publication_date <- as.numeric(books$publication_date) #This makes sure the resulting values are numeric
head(books$publication_date) #Checking to make sure we no longer have character strings as date values
typeof(books$publication_date) #Success!
```

### Renaming the dates column to something more convenient
```{r}
colnames(books)
newnames <- c("bookID", "title", "authors", "average_rating", "lang_code", "num_pages", "num_ratings", "num_text_reviews", "pub_date", "publisher")
colnames(books) <- newnames
colnames(books) #Checking to make sure the columns have the appropriate names
```

## Quick look at the cleaned data
```{r}
summary(books)
```

#### Without removing extreme cases, here is the baseline data:

##### The average rating for books in this data set is 3.934
##### The average number of pages for books in this data set is 299
##### The average number of ratings for books in this data set is 17,936
##### The average publication date for books in this data set is 2000, with the earliest being 1900 and the most recent being 2020. 

## Initial exploration of the data with some simple R code
```{r}
books[which.max(books$num_pages), ] #The book in the dataset with the most number of pages is a 5 volume set of the Complete Aubrey/Maturin Novels
```

#### Which book had the highest number of text reviews?
```{r}
books[which.max(books$num_text_reviews),] #Ahh lovely... Twilight...
```

#### Which book had the lowest rating?
```{r}
library(dplyr)
book_rated <- books %>% filter(num_ratings>0)

book_rated[which.min(book_rated$average_rating),]
```

#### Which book had the highest rating?
```{r}
book_extra_rated <- books %>% filter(num_ratings>100)
book_extra_rated[which.max(book_extra_rated$average_rating),]
```


## Using the ggplot2 histogram function to look deeper into the dataset
```{r}
library(ggplot2)
library(gridExtra)

num_pages_plot <- ggplot(books, aes(num_pages)) + #The majority of books are under 500 pages in length
                    geom_histogram() +
                    labs(title="Book Lengths", x="Number of Pages per Book", y="Frequency") +
                    xlim(0,1500)
  
text_rev_plot <- ggplot(books, aes(num_text_reviews)) + #The vast majority of books have under 5000 text reviews
                    geom_histogram() +
                    labs(title="Number of Text Reviews", x="Number of Text Reviews per Book", y="Frequency") +
                    xlim(0,10000) +
                    ylim(0,600)

grid.arrange(num_pages_plot, text_rev_plot, ncol = 2)

```

##### There is an interesting similarity in these two histograms, suggesting a relationship between number of pages and number of text reviews for each book title.


```{r}

avg_ratings_plot <- ggplot(books, aes(average_rating)) + #Most books have an average rating somewhere between 3.5 and 4.5
                      geom_histogram() +
                      labs(title="Average Ratings", x="Average Ratings Per Book", y="Frequency") +
                      xlim(2.5,5)
avg_ratings_plot
```

##### The vast majority of titles had average ratings between 3.5 and 4.5, however outliers are present.


### Investigating relationships between the variables with ggplot2 scatter plots, using appropriate x and y-axis limitations to view the bulk of the data
*The various ylim and xlim values were largely obtained using the +-1.5(IQR) formula, using common sense when necessary.*

### At a quick glance doesn't appear to be a clear relationship between number of pages and average rating, however upon closer inspection some interesting trends emerge.

```{r}
ggplot(books, aes(x=num_pages, y=average_rating)) +
  geom_point() +
  xlim(0,1500) +
  ylim(2.5,5)
```

##### It can be noted that as the number of pages reaches 500 and above, the number of ratings appears to thin out. This suggests that fewer people were willing to either start or finish books with more page numbers, which makes a lot of sense. It is also interesting to note that despite having fewer overall ratings, the average ratings of books with page numbers above 500 appear to be more concentrated between 3.5 and 4.5, in line with the inference made from the average ratings histogram. This suggests that despite having fewer readers, these high-page number books were of a higher overall quality. 

```{r}
above <- ggplot(books, aes(x=num_pages, y=average_rating)) +
          geom_point() +
          xlim(500,1500) +
          ylim(2.5,5) +
          labs(title="Ratings > 500 Pages", x="Number of Pages per Book", y="Average Rating") +
          geom_hline(yintercept=3.5, color="red") +
          geom_hline(yintercept=4.5, color="red")
below <- ggplot(books, aes(x=num_pages, y=average_rating)) +
          geom_point() +
          xlim(0,500) +
          ylim(2.5, 5) +
          labs(title="Ratings < 500 Pages", x="Number of Pages per Book", y="Average Rating") +
          geom_hline(yintercept=3.5, color="red") +
          geom_hline(yintercept=4.5, color="red")

grid.arrange(below, above, ncol = 2)
```

##### This trend can be viewed more easily with the visualization above, which compares the average ratings of books under 500 pages and over 500 pages.It is now clear that books with over 500 pages are much more likely to have an average rating between 3.5 and 4.5 than books with fewer than 500 pages.

##### Off the top of my head, two book series that would fit this criteria of high page number, fewer readers and yet higher average ratings are Tolkien's Lord of the Rings, and J.K. Rowling's Harry Potter series. These books are very long and take patience to read, however are widely acclaimed as fantastic series that have shaped popular culture. 

```{r}
library(sqldf)
sqldf("select title, average_rating, num_pages, num_text_reviews from books where authors like 'J.K. Rowling' and lang_code like 'eng' and num_pages > 500") 

sqldf("select title, average_rating, num_pages, num_text_reviews from books where authors like 'J.R.R. Tolkien' and lang_code like 'eng' and num_pages > 500 ")
```
##### These sql queries verify my assumption, showing that books both in the Harry Potter and Lord of the Rings series fit the trend identified in the previous visualizations. The results of these queries also reveal the fact that this data set not only includes individual books, but collections of books, which accounts for the abnormally high page numbers of certain entries also seen in the previous visualizations. This is an interesting finding, as I had previously assumed that the entries in this data set were all individual titles.

```{r}
sqldf("select title from books where num_pages > 1200")
```

##### Apart from a few titles like "Study Bible NIV", "The Second World War", and "The Decline and Fall of the Roman Empire", it appears that most books over 1200 pages in length in this data set are in fact collections, not individual titles. This is important to keep in mind, as the previous and future visualizations in this exploration may be misleading without this knowledge.


<!-- #### The only relationship that can be seen at a glance between book length and date of publication is that prior to the late 1950s, books and collections logged into GoodReads do not surpass 1000 pages. Most books stored in the GoodReads data set appear to be under 1000 pages in general. -->
<!-- ```{r} -->
<!-- ggplot(books, aes(num_pages, pub_date)) + -->
<!--   geom_point() + -->
<!--   xlim(0,1500) + -->
<!--   labs(title="Number of Book Pages by Publication Date", x="Number of Pages per Book", y="Publication Year") + -->
<!--   geom_hline(yintercept=1950, color="red") + -->
<!--   geom_vline(xintercept=1000, color="red") -->


<!-- ``` -->

## To further explore the dataset I will create an interactive multivariate plot, with language as the grouping variable. 
##### I have already identified a potential relationship between number of pages, number of text reviews, and average rating. Now I would like to see if there are any relationships between publishing language, publication date, number of pages, and the average_rating of books and collections in the dataset.
```{r}
unique(books$lang_code)
books$lang_code <- factor(books$lang_code, ordered=TRUE, levels=c("eng", "en-US", "fre", "spa", "en-GB", "mul", "grc", "enm", "en-CA", "ger", "jpn", "ara", "nl", 
                                                                  "zho", "lat", "por", "srp", "ita", "rus", "msa", "glg", "wel", "swe", "nor", "tur", "gla", "ale"))
```

### I only want lang_code, num_pages, average_rating, and pub_date, so I will eliminate all other columns and create a new dataframe called "mvbooks".
```{r}
mvbooks<-books[,-1:-3]
mvbooks<-mvbooks[,-7]
mvbooks<-mvbooks[,-4:-5]
colnames(mvbooks)
```
### Now the four desired variables are singled out in the "mvbooks" dataframe, however for convenience I want the grouping variable, "lang_code", on the far left of the multivariate plot.
```{r}
mvbooks2 <- mvbooks[,c(2,4,3,1)]
colnames(mvbooks2)
summary(mvbooks2)
```
### Now to plot the newly created "mvbooks2" dataframe.
```{r}
library(cdparcoord)
mm <- discretize(mvbooks2,nlevels=100) 
discparcoord(mm,k=5000,saveCounts=FALSE,name="test") 
```

### This plot is far too cluttered, and would be more useful if there were fewer years and page numbers smushed together.
```{r}
summary(mvbooks2$pub_date)
```

#### I will use the 1.5(IQR) strategy to figure out which years are considered likely outliers. The IQR of pub_date is 2005-1998, or 7 years. 7(1.5)=10.5. 2005+10.5=2010.5, or 2011 rounding up. 1998-10.5=1987.5, or 1987 rounding down. Let's just look at books published between 1987 and 2011.
```{r}
mvbooks3 <- mvbooks2 %>% filter(pub_date >= 1987, pub_date <= 2011)
summary(mvbooks3)
```

##### The same process will be repeated for the "num_pages" column.
```{r}
summary(mvbooks3$num_pages)
```

#### The IQR of num_pages is 220. 220(1.5)=330. 196, the 1st quartile, ends up as a negative number with 196-330= -134, so we can ignore that for now. 416, however, the 3rd quartile, leads to 416+330 = 746. Let's narrow down the books to those that have less than or equal to 746 pages.
```{r}
summary(mvbooks3$num_pages)
mvbooks3 <- mvbooks3 %>% filter(num_pages <= 746)
summary(mvbooks3)
```


### With the newly created "mvbooks3" dataframe, the multivariate visualization should be less cluttered.
```{r}
mm <- discretize(mvbooks3,nlevels=100) 
discparcoord(mm,k=5000,saveCounts=FALSE,name="test")
```

### While the plot is still fairly cluttered, I still made an interesting finding. I figured out that there is atleast 1 book in the dataset published in 1989, with fewer than 3 pages but an average GoodReads rating of 4 or greater. It appears that they were published in English. Very odd.

<iframe width="560" height="315" src="https://www.youtube.com/embed/Xz7OaMeybvM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen data-external="1"></iframe>

### Verifying the existence of the strange findings using an sql query
```{r echo=TRUE}
sqldf("select * from books where pub_date == 1989 and average_rating >= 4 and num_pages <= 3")
```

#### There are two books that meet this strange criteria, however double checking with the current GoodReads website reveals that these were entered with inaccurate values. The Day Before Midnight in reality has 434 pages according to google books, and the Feynman Lectures on Physics actually has 384 pages, according to GoodReads as of current. A careful analysis of the visualization, followed up by verification, lead to the discovery of an important error in the dataset. There are likely other such errors that were not caught as well.


### Just for fun, let's make a random book generator that uses titles from the GoodReads dataset we are working with
```{r}
gimmeABook <- function(){
  range <- c(1:11127)
  rando <- sample(range, 1)
  books[rando,]
}

gimmeABook()
gimmeABook()
gimmeABook()
gimmeABook()
```


#### *Through this exploration of the GoodReads data set in R, I have identified potential relationships between book length and average rating, as well as between book length and the number of text reviews. Books and collections with over 500 pages tend to have a more reliable average rating between 3.5 and 4.5 than books with less than 500 pages. In addition, books and collections with over 500 pages tend to have fewer total text reviews. This indicates that while fewer overall people are likely reading these longer books and collections, they are having more reliably positive experiences while reading them.*

#### *I have also identified two important pieces of information regarding the data set. One, the data includes both individual books and collections of books. This was verified by an sql query, which revealed that the vast majority of titles in the data set with more than 1200 pages are collections of books, rather than standalone publications. Two, there were likely many mistakes made by the creator of this data when entering the information. This was first discovered while exploring the multivariate plot created with the cdparcoord package, and then verified with sql queries. Two items in the data set were found to have incorrect information regarding page numbers. Because of this, an unknown number of titles in the data set could have false page number information. This could only be ameliorated through cross-checking each page number entry with outside sources.*

#### *While I have not exhausted the possibilities of analysis and manipulation of this data set, I have identified two important trends in the data using visual analysis. I have displayed a number of different pathways for analysis and visualization. SQL queries were implemented to check for findings in the visualizations, and the writing of functions were demonstrated to playfully explore the data set.*

#### *Thank you for reading,*

#### *--Greg*
